First we will look at regularization in the linear regression model then extend the idea into matrix factorization, in a similar manner. In linear regression we have $\hat{y}=w^Tx$. The idea of regularization is to prevent overfitting and allow for better generalization of the model. To do so, we add a penalizing term to the loss that penalizes larger weights. There are multiple ways of doing regularization for example L1 or L2. We will look at L2.

The new loss function under L2 regularization, also called Ridge regularization is:
$$L=\sum_{i=1}^n(y_{i}-\hat{y}_i)^2+\lambda||w||^2_2$$
Where $||w||^2_2$ is the 2-norm (Euclidean norm) squared of $w$.
It can be shown that minimizing this loss functions has the solution:
$$w=(X^TX+\lambda I)^{-1}X^Ty$$
https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html
# Extend to Matrix Factorization
We can define a new loss function and penalize large weights in the same manner:
$$L=\sum_{i,j\in\Omega}(r_{ij}-\hat{r}_{ij})^2+\lambda(||W||^2_F+||U||^2_F+||b||^2_2+||c||^2_2)$$
Where $||\cdot||_F$ is the Frobenius norm of a matrix, which is the square root of the sum of all elements squared of a matrix, essentially an extension of the 2-norm into matrices.
